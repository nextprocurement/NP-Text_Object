{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9730fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c725ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lbartolome/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4249e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding(text_list):\n",
    "    return [model.encode(text) for text in text_list]\n",
    "\n",
    "def extract_heading_text(xml_content):\n",
    "    try:\n",
    "        # Parsear el contenido XML para crear un árbol de elementos\n",
    "        root = etree.fromstring(xml_content)\n",
    "        \n",
    "        # Lista para guardar textos de las etiquetas 'heading'\n",
    "        heading_texts = []        \n",
    "        # Recorrer todos los elementos y extraer textos de las etiquetas 'heading'\n",
    "        for element in root.iter('heading'):\n",
    "            # Agregar el texto de la etiqueta 'heading' a la lista\n",
    "            heading_texts.append(element.text.strip() if element.text else \"\")\n",
    "        \n",
    "        return heading_texts\n",
    "    except etree.XMLSyntaxError:\n",
    "        print(\"Error al parsear xml\")\n",
    "        return []  \n",
    "    \n",
    "def extract_all_xml_tags(xml_content):\n",
    "    try:\n",
    "        # Parsear el contenido XML para crear un árbol de elementos\n",
    "        root = etree.fromstring(xml_content)\n",
    "        # Inicializar una lista para guardar solo los nombres de las etiquetas\n",
    "        all_tags = []\n",
    "        # Recorrer todos los elementos en el árbol XML\n",
    "        for element in root.iter():\n",
    "            all_tags.append(element.tag)\n",
    "    \n",
    "        return all_tags\n",
    "    \n",
    "    except etree.XMLSyntaxError:\n",
    "        print(\"Error al procesar\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0116a731-2aae-497f-b923-659b3fea0a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "def parse_xml_from_bytes(data_bytes):\n",
    "    try:\n",
    "        # Parsear el XML directamente desde bytes\n",
    "        root = etree.fromstring(data_bytes)\n",
    "        # Para extraer todo el texto concatenado de los elementos XML\n",
    "        return ''.join(root.xpath('//text()'))\n",
    "    except Exception as e:\n",
    "        print(f\"Error al parsear XML: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19088fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:   0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['procurement_id', 'doc_name', 'content', 'alternative_lang',\n",
      "       'translated_content', 'extracted_tags', 'texto_heading', 'extracted'],\n",
      "      dtype='object')\n",
      "Archivo procesado: /export/data_ml4ds/NextProcurement/PLACE/pdf2txt_ca_translated/procurements_file_15_containing_252064_docs_mark_fixed.parq en 82.64174365997314 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:   6%|▌         | 1/17 [02:10<34:44, 130.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['procurement_id', 'doc_name', 'content', 'alternative_lang',\n",
      "       'translated_content', 'extracted_tags', 'texto_heading', 'extracted'],\n",
      "      dtype='object')\n",
      "Archivo procesado: /export/data_ml4ds/NextProcurement/PLACE/pdf2txt_ca_translated/procurements_file_2_containing_252000_docs_mark_fixed.parq en 81.29951477050781 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  12%|█▏        | 2/17 [04:20<32:30, 130.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['procurement_id', 'doc_name', 'content', 'alternative_lang',\n",
      "       'translated_content', 'extracted_tags', 'texto_heading', 'extracted'],\n",
      "      dtype='object')\n",
      "Archivo procesado: /export/data_ml4ds/NextProcurement/PLACE/pdf2txt_ca_translated/procurements_file_7_containing_250227_docs_mark_fixed.parq en 80.42885446548462 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  18%|█▊        | 3/17 [06:59<33:26, 143.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['procurement_id', 'doc_name', 'content', 'alternative_lang',\n",
      "       'translated_content', 'extracted_tags', 'texto_heading', 'extracted'],\n",
      "      dtype='object')\n",
      "Archivo procesado: /export/data_ml4ds/NextProcurement/PLACE/pdf2txt_ca_translated/procurements_file_10_containing_251208_docs_mark_fixed.parq en 75.38251066207886 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  24%|██▎       | 4/17 [09:32<31:51, 147.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['procurement_id', 'doc_name', 'content', 'alternative_lang',\n",
      "       'translated_content', 'extracted_tags', 'texto_heading', 'extracted'],\n",
      "      dtype='object')\n",
      "Archivo procesado: /export/data_ml4ds/NextProcurement/PLACE/pdf2txt_ca_translated/procurements_file_11_containing_252661_docs_mark_fixed.parq en 77.98611164093018 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  29%|██▉       | 5/17 [12:10<30:14, 151.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['procurement_id', 'doc_name', 'content', 'alternative_lang',\n",
      "       'translated_content', 'extracted_tags', 'texto_heading', 'extracted'],\n",
      "      dtype='object')\n",
      "Archivo procesado: /export/data_ml4ds/NextProcurement/PLACE/pdf2txt_ca_translated/procurements_file_5_containing_250098_docs_mark_fixed.parq en 75.40749073028564 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  35%|███▌      | 6/17 [14:39<27:33, 150.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['procurement_id', 'doc_name', 'content', 'alternative_lang',\n",
      "       'translated_content', 'extracted_tags', 'texto_heading', 'extracted'],\n",
      "      dtype='object')\n",
      "Archivo procesado: /export/data_ml4ds/NextProcurement/PLACE/pdf2txt_ca_translated/procurements_file_13_containing_250951_docs_mark_fixed.parq en 78.9637541770935 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  41%|████      | 7/17 [17:11<25:10, 151.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['procurement_id', 'doc_name', 'content', 'alternative_lang',\n",
      "       'translated_content', 'extracted_tags', 'texto_heading', 'extracted'],\n",
      "      dtype='object')\n",
      "Archivo procesado: /export/data_ml4ds/NextProcurement/PLACE/pdf2txt_ca_translated/procurements_file_8_containing_251687_docs_mark_fixed.parq en 78.07225012779236 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  47%|████▋     | 8/17 [19:46<22:49, 152.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['procurement_id', 'doc_name', 'content', 'alternative_lang',\n",
      "       'translated_content', 'extracted_tags', 'texto_heading', 'extracted'],\n",
      "      dtype='object')\n",
      "Archivo procesado: /export/data_ml4ds/NextProcurement/PLACE/pdf2txt_ca_translated/procurements_file_1_containing_251067_docs_mark_fixed.parq en 77.60586380958557 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  53%|█████▎    | 9/17 [22:20<20:21, 152.72s/it]"
     ]
    }
   ],
   "source": [
    "# Ruta donde se encuentran los archivos Parquet originales a leer\n",
    "path = '/export/data_ml4ds/NextProcurement/PLACE/pdf2txt_ca_translated'\n",
    "# Ruta donde se guardarán los archivos Parquet procesados\n",
    "output_path = '/export/data_ml4ds/NextProcurement/PLACE/BSC_procesados_ca'\n",
    "\n",
    "# Crear la lista de todos los archivos .parq en el directorio especificado\n",
    "all_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.parq')]\n",
    "\n",
    "palabras_clave = ['tecnicas']\n",
    "# Expresión regular para identificar palabra clave\n",
    "regex = '|'.join(palabras_clave)\n",
    "\n",
    "# Usar tqdm en el bucle externo para ver el progreso en el procesamiento de archivos\n",
    "for file in tqdm(all_files, desc=\"Procesando archivos\"):\n",
    "    # Leer el archivo Parquet\n",
    "    df = pd.read_parquet(file)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Filtrado del df para filas donde la columna 'doc_name' contenga palabra clave\n",
    "    df = df[df['doc_name'].str.contains(regex, na=False, case=False)]\n",
    "    # Añadir una columna para identificar secciones del xml y meterlo en una columna llamada secciones.\n",
    "    df['extracted_tags'] = df['content'].apply(extract_all_xml_tags)\n",
    "    df['texto_heading'] = df['content'].apply(extract_heading_text)\n",
    "    df['extracted'] = df['content'].apply(parse_xml_from_bytes)\n",
    "    # Filtrar las filas donde la lista 'texto_heading' está vacía\n",
    "    df = df[df['texto_heading'].map(len) > 0]\n",
    "\n",
    "    # Usar tqdm en el apply para ver el progreso del cálculo de embeddings\n",
    "    #tqdm.pandas(desc=\"Calculando embeddings\")\n",
    "    #df['embeddings_heading'] = df['texto_heading'].progress_apply(calculate_embedding)\n",
    "    print(df.columns)\n",
    "    df = df[['procurement_id', 'doc_name', 'extracted', 'extracted_tags', 'texto_heading', \"translated_content\"]]#'embeddings_heading'\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Archivo procesado: {file} en {end_time - start_time} segundos\")\n",
    "    \n",
    "    # Construir el nombre del nuevo archivo conservando el nombre original pero cambiando la ruta\n",
    "    new_file_path = os.path.join(output_path, os.path.basename(file))\n",
    "    # Guardar el DataFrame en la nueva ruta\n",
    "    df.to_parquet(new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b298624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
