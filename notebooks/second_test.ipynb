{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7fbfdad-9c73-4a35-a8c0-1e7791bcb343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional, Union\n",
    "import dspy\n",
    "import pickle\n",
    "from dspy.datasets import Dataset\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dspy.teleprompt import BootstrapFewShot, BootstrapFewShotWithRandomSearch\n",
    "import time\n",
    "from fuzzywuzzy import process\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e162c6b0-e15c-48fa-930c-0db065756218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bb2fcbd-e3c5-4454-a038-6aa40ede4b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/usuarios_ml4ds/lbartolome/NextProcurement/NP-Search-Tool/.env\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# API KEY #\n",
    "###########\n",
    "path_env = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/NextProcurement/NP-Search-Tool/.env\")\n",
    "print(path_env)\n",
    "load_dotenv(path_env)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "llm = dspy.OpenAI(model=\"gpt-3.5-turbo\")# \"gpt-4o-2024-05-13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37c8d33c-44ab-450a-95bc-c7454ea4c97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm = dspy.HFClientTGI(model=\"meta-llama/Meta-Llama-3-8B \", port=8080, url=\"http://127.0.0.1\")\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "387384e8-2c92-4105-9367-d0ffb06738c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'procurement_id', 'doc_name', 'text', 'objetivo'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_samples = pd.read_excel(\"data/admin_eval_task/curated/tarea_zaragoza.xlsx\")\n",
    "print(df_samples.columns)\n",
    "df_samples.columns = [\"idx\",\"procurement_id\", \"doc_name\", \"text\", \"objetivo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6263d19d-0e55-4841-95e9-086e6a134f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['idx', 'procurement_id', 'doc_name', 'text', 'objetivo'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87f81c16-7349-470a-bf74-f785f3608699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TenderDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_fpath: Union[pathlib.Path, str],\n",
    "        dev_size: Optional[float] = 0.2,  \n",
    "        test_size: Optional[float] = 0.2,\n",
    "        text_key: str = \"text\",\n",
    "        label_key: str = \"objetivo\",\n",
    "        seed: Optional[int] = 11235,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.labels = []\n",
    "        self._train = []\n",
    "        self._dev = []\n",
    "        self._test = []\n",
    "\n",
    "        # Read the training data\n",
    "        train_data = pd.read_excel(data_fpath)\n",
    "\n",
    "        train_data, temp_data = train_test_split(train_data, test_size=dev_size + test_size, random_state=seed)\n",
    "        dev_data, test_data = train_test_split(temp_data, test_size=test_size / (dev_size + test_size), random_state=seed)\n",
    "       \n",
    "        self._train = [\n",
    "            dspy.Example({**row}).with_inputs(text_key) for row in self._convert_to_json(train_data)\n",
    "        ]\n",
    "        self._dev = [\n",
    "            dspy.Example({**row}).with_inputs(text_key) for row in self._convert_to_json(dev_data)\n",
    "        ]\n",
    "        self._test = [\n",
    "            dspy.Example({**row}).with_inputs(text_key) for row in self._convert_to_json(test_data)\n",
    "        ]\n",
    "\n",
    "    def _convert_to_json(self, data: pd.DataFrame):\n",
    "        if data is not None:\n",
    "            return data.to_dict(orient='records')\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37953a2a-fa02-4cf3-b439-446f26f7b233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = TenderDataset(\n",
    "    data_fpath=\"data/admin_eval_task/curated/tarea_zaragoza.xlsx\",\n",
    "    dev_size=0.25,\n",
    ")\n",
    "\n",
    "trainset = dataset._train\n",
    "devset = dataset._dev\n",
    "testset = dataset._test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62bd5008-74be-4091-aefc-8aa9bfbb3ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Detecting language...\n",
      "-- -- Language detect finished in 3.994781255722046 seconds\n",
      "['DOCUMENTOS DEL CONTRATO', 'EXPEDIENTE DE CONTRATACIÓN:', 'OBLIGACIONES DEL CONTRATO', 'IV. EXECUCIÓ DEL CONTRACTE', 'CATALOGACIÓN EN EL CONTRATO', 'PRESTACIONES DEL CONTRATO', 'ÓRGANO DE CONTRATACIÓN PROPONENTE:', 'OBJETO DE LA CONTRATACIÓN', 'CATEGORÍA DEL CONTRATO', 'EJECUCIÓN DEL CONTRATO', 'ESPECIFICACIONES DEL CONTRATO', 'DESCRIPCIÓN DE LA CONTRATACIÓN', 'OBJETIVOS DEL CONTRATO.', 'COMISIÓN DE CONTRATACIÓN:', 'PROCEDIMIENTO DE CONTRATACIÓN', \"ÁMBIT D'APLICACIÓ DEL CONTRACTE\", 'DESCRIPCIÓN DE LOS CONTRATOS', 'ADJUDICACION DEL CONTRATO', 'REPRESENTANTE DEL CONTRATO', 'MODIFICACIÓN DEL CONTRATO.']\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def get_lang(df: pd.DataFrame, col_calculate_on: str) -> pd.DataFrame:\n",
    "    def det(x: str) -> str:\n",
    "        try:\n",
    "            lang = detect(x)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            lang = 'Other'\n",
    "        return lang\n",
    "\n",
    "    print(f\"-- Detecting language...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    df['lang'] = df[col_calculate_on].apply(det)\n",
    "\n",
    "    print(f'-- -- Language detect finished in {(time.time() - start_time)} seconds')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load final_labels from the pickle file\n",
    "with open('final_labels.pkl', 'rb') as file:\n",
    "    final_labels = pickle.load(file)\n",
    "\n",
    "# Convert final_labels to a DataFrame\n",
    "df_labels = pd.DataFrame(final_labels, columns=['label'])\n",
    "\n",
    "# Detect the language of each label\n",
    "df_labels = get_lang(df_labels, 'label')\n",
    "\n",
    "# Filter the DataFrame to keep only Spanish labels\n",
    "df_spanish_labels = df_labels[df_labels['lang'] != 'ca']\n",
    "\n",
    "# Convert the filtered DataFrame back to a list, if necessary\n",
    "spanish_labels = df_spanish_labels['label'].tolist()\n",
    "\n",
    "# Display the filtered Spanish labels\n",
    "#print(spanish_labels)\n",
    "\n",
    "# To use as HINT\n",
    "def normalize_string(s):\n",
    "    # Convert to lower case\n",
    "    s = s.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    # Remove extra spaces\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "normalized_array = np.array([normalize_string(item) for item in spanish_labels])\n",
    "unique_dict = {}\n",
    "for original, normalized in zip(final_labels, normalized_array):\n",
    "    if normalized not in unique_dict:\n",
    "        unique_dict[normalized] = original\n",
    "clean_final_labels = np.array(list(unique_dict.values()))\n",
    "\n",
    "\n",
    "# Function to normalize the list\n",
    "def normalize_labels(labels, threshold=85):\n",
    "    normalized = []\n",
    "    for label in labels:\n",
    "        match = process.extractOne(label, normalized, score_cutoff=threshold)\n",
    "        if match:\n",
    "            normalized.append(match[0])\n",
    "        else:\n",
    "            normalized.append(label)\n",
    "    return list(set(normalized))\n",
    "\n",
    "# Normalize the labels\n",
    "normalized_labels = normalize_labels(clean_final_labels)\n",
    "print(normalized_labels)\n",
    "\n",
    "final_labels = [\"EJECUCIÓN DEL CONTRATO\", \"DESCRIPCIÓN DE LOS CONTRATOS\", \"OBJETIVOS DEL CONTRATO\", \"OBLIGACIONES DEL CONTRATO\", \"OBJETO DE LA CONTRATACIÓN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fca0b77-c4dd-4fc8-ab5c-46562740810c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PredictObjecto(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Extract the objective of the contract from a document containing the technical specifications of a Spanish public tender. If the objective is not present in the document, return '/'.\n",
    "\n",
    "    Requirements:\n",
    "\n",
    "    The extracted text must exclusively consist of words from the document. No additional words are allowed.\n",
    "    The language of the document must remain unchanged under all circumstances.\n",
    "    \"\"\"\n",
    "\n",
    "    TENDER = dspy.InputField(desc=\"The document containing the technical specifications of the Spanish public tender.\")\n",
    "    OBJECTIVE = dspy.OutputField(desc=\"The tender objective, or 'N_A' if not present.\")\n",
    "\n",
    "\n",
    "class PredictModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predict = dspy.Predict(PredictObjecto)\n",
    "        #self.predict = dspy.ChainOfThoughtWithHint(PredictObjecto)\n",
    "        \n",
    "    def _process_output(self, text):\n",
    "        \n",
    "        if \"N_A\" in text:\n",
    "            return \"/\"\n",
    "        else:\n",
    "            return text\n",
    "      \n",
    "    def forward(self, text):\n",
    "        #hint = f\"Valid candidates for 'tender objective' may start with {final_labels}.\"\n",
    "\n",
    "        pred = self.predict(TENDER=text[0:5000])#, hint=hint\n",
    "\n",
    "        return dspy.Prediction(objective=self._process_output(pred.OBJECTIVE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a989863-adb3-4ae0-b827-65514725f28d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combined_score(example, pred, trace=None):\n",
    "    def matching_score(example, pred, trace=None):\n",
    "        if example.objetivo == \"/\":\n",
    "            if pred[\"objective\"] == \"/\":\n",
    "                return 1.0\n",
    "            else:\n",
    "                return 0.0\n",
    "        \n",
    "        predicted_lst = pred[\"objective\"].split()\n",
    "        gt_lst = example.objetivo.split()\n",
    "        \n",
    "        predicted_set = set(predicted_lst)\n",
    "        gt_set = set(gt_lst)\n",
    "\n",
    "        intersection = predicted_set.intersection(gt_set)\n",
    "        union = predicted_set.union(gt_set)\n",
    "        \n",
    "        if len(union) == 0:\n",
    "            return 0.0\n",
    "        jaccard_similarity = len(intersection) / len(union)\n",
    "        \n",
    "        return jaccard_similarity\n",
    "\n",
    "    def is_in_text_score(example, pred, trace=None):\n",
    "        if example.objetivo == \"/\":\n",
    "            if pred[\"objective\"] == \"/\":\n",
    "                return 1.0\n",
    "            else:\n",
    "                return 0.0\n",
    "            \n",
    "        text_lst = example.text[0:5000].lower().split()\n",
    "        predicted_lst = pred[\"objective\"].lower().split()\n",
    "\n",
    "        words_not_in_text = [word for word in predicted_lst if word not in text_lst]\n",
    "        num_words_not_in_text = len(words_not_in_text)\n",
    "        \n",
    "        total_predicted_words = len(predicted_lst)\n",
    "        score = max(0.0, 1.0 - (num_words_not_in_text / total_predicted_words))\n",
    "        \n",
    "        return score\n",
    "\n",
    "    match_score = matching_score(example, pred, trace)\n",
    "    text_score = is_in_text_score(example, pred, trace)    \n",
    "    combined = (0.5 * match_score) + (0.5 * text_score)\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "004b2daf-9dc5-48d8-97c1-c76981a94b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 3 traces per predictor.\n",
      "Will attempt to train 10 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.722383986214217 / 18  (48.5): 100%|██████████| 18/18 [00:00<00:00, 275.38it/s]\n",
      "/home/lbartolome/.local/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:137: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.722383986214217 / 18  (48.5%)\n",
      "Score: 48.46 for set: [0]\n",
      "New best score: 48.46 for seed -3\n",
      "Scores so far: [48.46]\n",
      "Best score: 48.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.722383986214217 / 18  (48.5): 100%|██████████| 18/18 [00:00<00:00, 228.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.722383986214217 / 18  (48.5%)\n",
      "Score: 48.46 for set: [3]\n",
      "Scores so far: [48.46, 48.46]\n",
      "Best score: 48.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4/41 [00:00<00:00, 302.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.490846945163335 / 18  (69.4): 100%|██████████| 18/18 [00:00<00:00, 173.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.490846945163335 / 18  (69.4%)\n",
      "Score: 69.39 for set: [3]\n",
      "New best score: 69.39 for seed -1\n",
      "Scores so far: [48.46, 48.46, 69.39]\n",
      "Best score: 69.39\n",
      "Average of max per entry across top 1 scores: 0.6939359413979631\n",
      "Average of max per entry across top 2 scores: 0.7141382826369798\n",
      "Average of max per entry across top 3 scores: 0.7141382826369798\n",
      "Average of max per entry across top 5 scores: 0.7141382826369798\n",
      "Average of max per entry across top 8 scores: 0.7141382826369798\n",
      "Average of max per entry across top 9999 scores: 0.7141382826369798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3/41 [00:00<00:00, 319.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.064963803603172 / 18  (44.8): 100%|██████████| 18/18 [00:00<00:00, 214.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.064963803603172 / 18  (44.8%)\n",
      "Score: 44.81 for set: [3]\n",
      "Scores so far: [48.46, 48.46, 69.39, 44.81]\n",
      "Best score: 69.39\n",
      "Average of max per entry across top 1 scores: 0.6939359413979631\n",
      "Average of max per entry across top 2 scores: 0.7141382826369798\n",
      "Average of max per entry across top 3 scores: 0.7141382826369798\n",
      "Average of max per entry across top 5 scores: 0.7141382826369798\n",
      "Average of max per entry across top 8 scores: 0.7141382826369798\n",
      "Average of max per entry across top 9999 scores: 0.7141382826369798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/41 [00:00<00:00, 242.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.158840213201268 / 18  (12.0): 100%|██████████| 18/18 [00:00<00:00, 247.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.158840213201268 / 18  (12.0%)\n",
      "Score: 11.99 for set: [3]\n",
      "Scores so far: [48.46, 48.46, 69.39, 44.81, 11.99]\n",
      "Best score: 69.39\n",
      "Average of max per entry across top 1 scores: 0.6939359413979631\n",
      "Average of max per entry across top 2 scores: 0.7141382826369798\n",
      "Average of max per entry across top 3 scores: 0.7141382826369798\n",
      "Average of max per entry across top 5 scores: 0.722440769200336\n",
      "Average of max per entry across top 8 scores: 0.722440769200336\n",
      "Average of max per entry across top 9999 scores: 0.722440769200336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/41 [00:00<00:00, 223.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 18  (0.0): 100%|██████████| 18/18 [00:00<00:00, 270.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 18  (0.0%)\n",
      "Score: 0.0 for set: [3]\n",
      "Scores so far: [48.46, 48.46, 69.39, 44.81, 11.99, 0.0]\n",
      "Best score: 69.39\n",
      "Average of max per entry across top 1 scores: 0.6939359413979631\n",
      "Average of max per entry across top 2 scores: 0.7141382826369798\n",
      "Average of max per entry across top 3 scores: 0.7141382826369798\n",
      "Average of max per entry across top 5 scores: 0.722440769200336\n",
      "Average of max per entry across top 8 scores: 0.722440769200336\n",
      "Average of max per entry across top 9999 scores: 0.722440769200336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/41 [00:00<00:00, 211.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 10.005318268498243 / 18  (55.6): 100%|██████████| 18/18 [00:00<00:00, 185.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.005318268498243 / 18  (55.6%)\n",
      "Score: 55.59 for set: [3]\n",
      "Scores so far: [48.46, 48.46, 69.39, 44.81, 11.99, 0.0, 55.59]\n",
      "Best score: 69.39\n",
      "Average of max per entry across top 1 scores: 0.6939359413979631\n",
      "Average of max per entry across top 2 scores: 0.7123554389188552\n",
      "Average of max per entry across top 3 scores: 0.7162367275421139\n",
      "Average of max per entry across top 5 scores: 0.7162367275421139\n",
      "Average of max per entry across top 8 scores: 0.7232925801862019\n",
      "Average of max per entry across top 9999 scores: 0.7232925801862019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/41 [00:00<00:00, 242.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.9117667818115387 / 18  (10.6): 100%|██████████| 18/18 [00:00<00:00, 197.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.9117667818115387 / 18  (10.6%)\n",
      "Score: 10.62 for set: [3]\n",
      "Scores so far: [48.46, 48.46, 69.39, 44.81, 11.99, 0.0, 55.59, 10.62]\n",
      "Best score: 69.39\n",
      "Average of max per entry across top 1 scores: 0.6939359413979631\n",
      "Average of max per entry across top 2 scores: 0.7123554389188552\n",
      "Average of max per entry across top 3 scores: 0.7162367275421139\n",
      "Average of max per entry across top 5 scores: 0.7162367275421139\n",
      "Average of max per entry across top 8 scores: 0.7232925801862019\n",
      "Average of max per entry across top 9999 scores: 0.7232925801862019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3/41 [00:00<00:00, 303.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.50928988814715 / 18  (63.9): 100%|██████████| 18/18 [00:00<00:00, 189.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.50928988814715 / 18  (63.9%)\n",
      "Score: 63.94 for set: [3]\n",
      "Scores so far: [48.46, 48.46, 69.39, 44.81, 11.99, 0.0, 55.59, 10.62, 63.94]\n",
      "Best score: 69.39\n",
      "Average of max per entry across top 1 scores: 0.6939359413979631\n",
      "Average of max per entry across top 2 scores: 0.706717118197745\n",
      "Average of max per entry across top 3 scores: 0.7164636330492901\n",
      "Average of max per entry across top 5 scores: 0.7192271482861579\n",
      "Average of max per entry across top 8 scores: 0.7262830009302459\n",
      "Average of max per entry across top 9999 scores: 0.7262830009302459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4/41 [00:00<00:00, 286.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.355477442182027 / 18  (46.4): 100%|██████████| 18/18 [00:00<00:00, 132.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.355477442182027 / 18  (46.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 46.42 for set: [3]\n",
      "Scores so far: [48.46, 48.46, 69.39, 44.81, 11.99, 0.0, 55.59, 10.62, 63.94, 46.42]\n",
      "Best score: 69.39\n",
      "Average of max per entry across top 1 scores: 0.6939359413979631\n",
      "Average of max per entry across top 2 scores: 0.706717118197745\n",
      "Average of max per entry across top 3 scores: 0.7164636330492901\n",
      "Average of max per entry across top 5 scores: 0.7192271482861579\n",
      "Average of max per entry across top 8 scores: 0.7269146403215752\n",
      "Average of max per entry across top 9999 scores: 0.7269146403215752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/41 [00:00<00:00, 252.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.592719712196011 / 18  (47.7): 100%|██████████| 18/18 [00:00<00:00, 145.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.592719712196011 / 18  (47.7%)\n",
      "Score: 47.74 for set: [3]\n",
      "Scores so far: [48.46, 48.46, 69.39, 44.81, 11.99, 0.0, 55.59, 10.62, 63.94, 46.42, 47.74]\n",
      "Best score: 69.39\n",
      "Average of max per entry across top 1 scores: 0.6939359413979631\n",
      "Average of max per entry across top 2 scores: 0.706717118197745\n",
      "Average of max per entry across top 3 scores: 0.7164636330492901\n",
      "Average of max per entry across top 5 scores: 0.7192271482861579\n",
      "Average of max per entry across top 8 scores: 0.7386186703997035\n",
      "Average of max per entry across top 9999 scores: 0.7456745230437914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/41 [00:00<00:00, 308.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.070690391180063 / 18  (44.8): 100%|██████████| 18/18 [00:00<00:00, 300.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.070690391180063 / 18  (44.8%)\n",
      "Score: 44.84 for set: [3]\n",
      "Scores so far: [48.46, 48.46, 69.39, 44.81, 11.99, 0.0, 55.59, 10.62, 63.94, 46.42, 47.74, 44.84]\n",
      "Best score: 69.39\n",
      "Average of max per entry across top 1 scores: 0.6939359413979631\n",
      "Average of max per entry across top 2 scores: 0.706717118197745\n",
      "Average of max per entry across top 3 scores: 0.7164636330492901\n",
      "Average of max per entry across top 5 scores: 0.7192271482861579\n",
      "Average of max per entry across top 8 scores: 0.7479705141044938\n",
      "Average of max per entry across top 9999 scores: 0.7550263667485817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4/41 [00:00<00:00, 305.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.656283367650822 / 18  (37.0): 100%|██████████| 18/18 [00:00<00:00, 154.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.656283367650822 / 18  (37.0%)\n",
      "Score: 36.98 for set: [3]\n",
      "Scores so far: [48.46, 48.46, 69.39, 44.81, 11.99, 0.0, 55.59, 10.62, 63.94, 46.42, 47.74, 44.84, 36.98]\n",
      "Best score: 69.39\n",
      "Average of max per entry across top 1 scores: 0.6939359413979631\n",
      "Average of max per entry across top 2 scores: 0.706717118197745\n",
      "Average of max per entry across top 3 scores: 0.7164636330492901\n",
      "Average of max per entry across top 5 scores: 0.7192271482861579\n",
      "Average of max per entry across top 8 scores: 0.7479705141044938\n",
      "Average of max per entry across top 9999 scores: 0.7550263667485817\n",
      "13 candidate programs found.\n"
     ]
    }
   ],
   "source": [
    "config = dict(max_bootstrapped_demos=3, max_labeled_demos=3, num_candidate_programs=10, max_rounds=1,)\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=combined_score, **config)\n",
    "compiled_classifier = teleprompter.compile(PredictModule(), trainset=trainset, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "701571e0-61b0-43d9-9dd5-9f448da7d40b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tests = []\n",
    "for el in testset:\n",
    "    output = compiled_classifier(el.text)\n",
    "    tests.append([el.text[0:5000],el.objetivo, output[\"objective\"], combined_score(el,output)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e5e7c-541d-4b6e-b5c8-35db2a4db9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/deberta-xlarge-mnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c2dff0d-4eab-43a3-b7de-082f61c25d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(tests, columns=[\"TEXT\",\"GROUND\", \"PREDICTED\", \"METRIC\"])\n",
    "\n",
    "P, R, F1 = score(results.PREDICTED.values.tolist(), results.GROUND.values.tolist(), lang='es', model_type=model_name) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecf1d891-51f4-46fb-a2f9-502d7baa6497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7199627557398761"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"METRIC\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb05f0f7-161f-4f2c-8640-04f86aff0d88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8102), tensor(0.7834), tensor(0.7925))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.mean(), R.mean(), F1.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prueba",
   "language": "python",
   "name": "prueba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
